【要点まとめ】  
ある時系列データを別の時系列データに変換する手法に、Seq2Seqがある。  
入力のモジュールはEncoder、出力のモジュールはDecoderと呼ばれる。
機械翻訳、音声認識、対話アプリなどに用いられる。  
  
EncoderRNNでは、以下のステップが実施される。  
* Taking：文章を単語などのトークンに分割し、IDを振る
* Embedding：IDから、そのトークンを表現する分散表現ベクトルに変換する
IDは総数数十万を超えることもあるが、Embeddingで数百程度のベクトルに収まることが多い。  
Embeddingで元の単語に対して同じような単語は似たようなベクトルにしていくのが難しい…特徴量抽出という。  
※GoogleのBERTが成功例として有名。  
  
オートエンコーダは教師なし学習で、次元削減が可能。  
ただ、近い意味の単語は類似のベクトルにするという目的からは外れてしまう。それを解消するためにVAEが生み出された。  
  
【実装演習】  
演習チャレンジのencodeを実装  
```
def encode(words, E, W, U, b):
    hidden_size = W.shape[0]
    h = np.zeros(hidden_size)

    for w in words:
        e = E.dot(w)
        h = _activation(W.dot(e) + U.dot(h) + b) # 何らかの活性化関数
    return h
```
  
【実装演習考察】  
wがone-hotベクトルであることから、埋め込み行列Eとの掛け合わせで特徴量の変換が可能でした。  
演習チャレンジテストでしたが、これまでの学習から正答を予想することができました。  

【要点まとめ】  
Seq2Seqの問題点として、文章の長さがどうであれ、固定長のベクトルに収めなければならない。  
それを解決するために、文章が長くなるほど内部表現の次元も大きくなるAttentionalMechanismが考案された。  
* AttentionalMechanism：入力と出力のどの単語が関連しているかの関連度を学習する仕組み  
  
【実装演習】  
ソースコードが無いため省略します  
  
【実装演習考察】  
ソースコードが無いため省略します  
  
【自己学習】  
※参考資料：ゼロから作るDeepLearning②  
Attentionを用いることで、Seq2Seqは人間と同じように、必要な情報に注意を向けることができる。それは即ち「Attentionによって、人間がモデルの行う処理を理解できるようになった」と言える。それにより、これまでブラックボックスだったモデル内部の動きが人間の意図した動きをしているか、判断できるようになった。  

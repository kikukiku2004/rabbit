【要点まとめ】

●情報理論
点が散りばめられた2枚の図をパッと見たときに、どちらの点が多いか判別する際
点の増加量は同じなのに即判別可能な場合と判別が難しい場合がある。
もしかして、点の増加量ではなく比率で捉えているのではないかという考え方がある。
⊿w/w = 1/10 ←こちらは気づきづらい
⊿w/w = 1/1

自己情報量は「確率の関数」かつ「減少関数」かつ「加法性」を同時に満たす必要があるため、以下の式で定義される
I(x) = -log2P(x)

エントロピー（情報量を数値化したもの…元は熱力学用語）には以下のような種類がある
シャノンエントロピー（平均情報量）：自己情報量の平均値（期待値と言ってもよい）
カルバック・ライブラー・ダイバージェンス：２つの確率分布（P・Q）がどの程度似ているかを表す尺度
交差エントロピー：KLダイバージェンスの一部を取り出した、やはり確率分布（P・Q）の近似性を表現

【考察・課題】
自己情報量については概念は理解できた気がするが、実際の式が意味するところが
深く理解できていないので、今後さらに学習が必要。
※先生が、「logを扱う関数は情報量を扱ってるのかと疑ってもいい」と仰っていたのはなるほどと思った。

交差エントロピーは分類問題の損失関数（交差エントロピー誤差）としてなんとなく使っていたが
中身をきちんと理解していなかったので良い機会をいただけたと思って、理解に努める必要がある。
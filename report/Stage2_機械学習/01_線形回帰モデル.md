【要点まとめ】

●線形回帰
線形とはざっくり説明すると「比例」のこと
y = ax + b（2次元）
z = ax + by + c（3次元）
n次元における超平面の方程式は以下となる
y = a0 + a1・x1 + a2・x2 + ... + a(n-1)・x(n-1)

※数列Σの形がでてきたら、ベクトルに直せないかということを考えるのが大事！
 逆にベクトルの形の式が理解しづらいなら、Σに戻してみて検証してみるのも有効
※また、一般的な表記方法について覚えておくことは重要（論文とかは当然それで書かれている）…太文字はベクトルを表す、など

また、ベクトルの形は常に意識したほうがよい。
実装時にエラーが発生した場合、まず疑うのはそれ（shapeで見ると良い）

●データの分割とモデルの汎化性能測定
手元に無いデータが入力されたときに適切に予測したいので、モデルを生成する際は手元の全てのデータを学習に使うのではなく
何割かのデータは検証用に残して学習させる。

●線形回帰モデル
残差平方和について
二乗誤差の総和をとる（なぜ二乗するのか、損失関数の設計につながるためこの手の疑問は持ち続けたほうがいい）
また、二乗損失は一般的に外れ値に弱い（外れ値との差が二乗されて跳ねるため。それらを克服する設計もある）

公式についてはテキストにも記載があるため、あらためてここに書くことは不要と考えるが
今後さらなる理解のために追記する必要がでてくるかもしれないので、その際は知識定着の意味でもここに書いていきたい。


【実装演習】
```
from sklearn.datasets import load_boston
from pandas import DataFrame
import numpy as np
```
```
boston = load_boston()
```
```
print(boston['DESCR'])
```
```
.. _boston_dataset:

Boston house prices dataset
---------------------------

**Data Set Characteristics:**  

    :Number of Instances: 506 

    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.

    :Attribute Information (in order):
        - CRIM     per capita crime rate by town
        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.
        - INDUS    proportion of non-retail business acres per town
        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
        - NOX      nitric oxides concentration (parts per 10 million)
        - RM       average number of rooms per dwelling
        - AGE      proportion of owner-occupied units built prior to 1940
        - DIS      weighted distances to five Boston employment centres
        - RAD      index of accessibility to radial highways
        - TAX      full-value property-tax rate per $10,000
        - PTRATIO  pupil-teacher ratio by town
        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
        - LSTAT    % lower status of the population
        - MEDV     Median value of owner-occupied homes in $1000's

    :Missing Attribute Values: None

    :Creator: Harrison, D. and Rubinfeld, D.L.

This is a copy of UCI ML housing dataset.
https://archive.ics.uci.edu/ml/machine-learning-databases/housing/


This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.

The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic
prices and the demand for clean air', J. Environ. Economics & Management,
vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics
...', Wiley, 1980.   N.B. Various transformations are used in the table on
pages 244-261 of the latter.

The Boston house-price data has been used in many machine learning papers that address regression
problems.   
     
.. topic:: References

   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.
   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.
```
```
df = DataFrame(data=boston.data, columns=boston.feature_names)
```
```
df['PRICE'] = np.array(boston.target)
```
```
df.head(10)
```
```
CRIM	ZN	INDUS	CHAS	NOX	RM	AGE	DIS	RAD	TAX	PTRATIO	B	LSTAT	PRICE
0	0.00632	18.0	2.31	0.0	0.538	6.575	65.2	4.0900	1.0	296.0	15.3	396.90	4.98	24.0
1	0.02731	0.0	7.07	0.0	0.469	6.421	78.9	4.9671	2.0	242.0	17.8	396.90	9.14	21.6
2	0.02729	0.0	7.07	0.0	0.469	7.185	61.1	4.9671	2.0	242.0	17.8	392.83	4.03	34.7
3	0.03237	0.0	2.18	0.0	0.458	6.998	45.8	6.0622	3.0	222.0	18.7	394.63	2.94	33.4
4	0.06905	0.0	2.18	0.0	0.458	7.147	54.2	6.0622	3.0	222.0	18.7	396.90	5.33	36.2
5	0.02985	0.0	2.18	0.0	0.458	6.430	58.7	6.0622	3.0	222.0	18.7	394.12	5.21	28.7
6	0.08829	12.5	7.87	0.0	0.524	6.012	66.6	5.5605	5.0	311.0	15.2	395.60	12.43	22.9
7	0.14455	12.5	7.87	0.0	0.524	6.172	96.1	5.9505	5.0	311.0	15.2	396.90	19.15	27.1
8	0.21124	12.5	7.87	0.0	0.524	5.631	100.0	6.0821	5.0	311.0	15.2	386.63	29.93	16.5
9	0.17004	12.5	7.87	0.0	0.524	6.004	85.9	6.5921	5.0	311.0	15.2	386.71	17.10	18.9
```
```
df['RM'].head()
```
```
0    6.575
1    6.421
2    7.185
3    6.998
4    7.147
Name: RM, dtype: float64
```
```
# =====================
# 単回帰分析
# =====================
# 説明変数
data = df.loc[:, ['RM']].values
```
```
data[0:5]
```
```
array([[6.575],
       [6.421],
       [7.185],
       [6.998],
       [7.147]])
```
```
# 目的変数
target = df.loc[:,'PRICE'].values
```
```
target[0:5]
```
```
array([24. , 21.6, 34.7, 33.4, 36.2])
```
```
from sklearn.linear_model import LinearRegression
```
```
# モデル生成
model = LinearRegression()
model.get_params()
```
```
{'copy_X': True,
 'fit_intercept': True,
 'n_jobs': None,
 'normalize': False,
 'positive': False}
```
```
# fitでパラメータ推定
model.fit(data,target)
```
```
# 部屋数1で推定（金額がマイナスはおかしい！そもそも学習データに部屋数1は存在しないので、こういった結果になる）
model.predict([[1]])
```
```
array([-25.5685118])
```
```
```
```
```

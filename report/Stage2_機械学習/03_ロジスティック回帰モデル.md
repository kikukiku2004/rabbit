【要点まとめ】

ロジスティック回帰モデル
分類問題で使用される
→分類問題：いくつかの入力（説明変数・特徴量と呼ばれる）から０or1の出力を得る

分類問題へのアプローチ方法は大きく3つ
  1. 識別的アプローチ：p(Ck|x)を直接モデル化　☆ロジスティック回帰はこれに当たる
  2. 生成的アプローチ：p(Ck)とp(x|Ck)をmodel化し、その後ベイズ定理を用いてp(Ck|x)を求める
  　　p(Ck|x) = p(Ck, x) / p(x) = p(x|Ck)・p(Ck) / p(x)
  3. 識別関数：f(x) > 0 → C=1、f(x) < 0 → C=0　☆サポートベクタマシンなどが該当する

  ■識別的アプローチについて詳しく
    線形回帰を考えても仕方がない
    x(T)w ∈ R ←実数全体をとる、欲しいyは0or1なので実数全体から0or1に潰す必要がある

    シグモイド関数
    ・入力は実数、出力は0〜1の値
    ・(クラス1に分類される)確率を表現
    ・単調増加関数

    σ(x) = 1 / 1 + exp(-ax)
    ・パラメータ(a)の値が変わると、シグモイド関数自体の形が変わる。例えばaを大きくするとステップ関数のような形になる。

    シグモイド関数の出力をY=1になる値に対応させる
    P(Y=1|x) = σ(w0 + w1x1 + w2x2 + ... + wmxm)
    最もわかりやすいのは、0.5以上の出力なら1、それ以外なら0と決めてしまうこと

    データからpを導き出す…尤度関数を最大化するパラメータを探す（最尤推定）
    線形回帰のように最小2乗法を求めるのは困難であるため、勾配降下法を使用する
    ※パラメータが更新されなくなった場合、それは勾配が0…すなわち現段階では最適な解が求められた

    勾配降下法ではnの値が大きいときに、オンメモリだとかなり厳しい。→確率的勾配降下法というアプローチもある。



【実装演習】
一等客室の女性の生存率を予測してみる（かなり高いはず）
```
import pandas as pd
from sklearn.linear_model import LogisticRegression

data = pd.read_csv('./data/titanic_train.csv')
```
```
# 欠損値チェック
data.isnull().sum()
```
```
PassengerId      0
Survived         0
Pclass           0
Name             0
Sex              0
Age            177
SibSp            0
Parch            0
Ticket           0
Fare             0
Cabin          687
Embarked         2
dtype: int64
```
```
# PClassとSexを使う
# Sexのmale,femaleを数値に変換
data['Sex2'] = data['Sex'].map({'female' : 0, 'male' : 1}).astype(int)

X = data.loc[:, ['Pclass', 'Sex2']]
y = data.loc[:, ['Survived']]
```
```
# 学習〜予測
model = LogisticRegression()
model.fit(X, y)

# Pclass:1（一等客室）、Sex2:0（female）
model.predict_proba([[1, 0]])
```
```
array([[0.09515191, 0.90484809]])
```
【考察】
先の予想どおり、高額な部屋の女性は生存率が高いであろう結果が得られた。
0or1の確率が得られるのは確かに、際どいラインの結果では一旦保留という手が使えるように感じた。
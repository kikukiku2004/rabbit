【要点まとめ】

●単回帰/重回帰
非線形な回帰式のxの代わりにφ(x)を用いる。
※wについては線形のまま！（何について線形なのか、何について非線形なのかが重要！）
※線形モデルによる非線形回帰である。
（linear-in-parameter）

yi = w0 + w1・φ1(xi) + w2・φ2(xi) + ... + w9・φ9(xi)
   = w0 + w1・xi + w2・xi^2 + ... + w9・xi^9　←求めるべきwについては線形！


●未学習と過学習
・未学習の対策は、次元数を増やすしかない
・過学習の対策は
　1. 学習データの数を増やす（数の暴力）
　2. 基底関数を削除して表現力を抑止（特徴量の取捨選択）
　3. 正則化法を利用して表現力を抑止（モデルが複雑になれば罰則項を適用していく）

  ■正則化法について詳しく
  　予測：y(hat) = X*・(X(T)X)^-1・X(T)y
        X = [[1,2,4],[1,3,5.9],[1,4,8.1]] …ほぼ平行であるため、(X(T)X)^-1、すなわちW(hat)の要素はとても大きくなる
        そこでW(hat)の大きさに比例した罰則を与える
        E(W) = J(W) + λ・W(T)W　← J(W)はMSE（これが小さくなるようにWを考える）、すなわち+以降（罰則項）をなるべく小さく抑えたい
        解きたいのは、min MSE s.t. R(W)<=r

    基底関数が多くなると一般的には過学習してしまうが、正則化法を入れることによってある程度緩和することができる（滑らかな曲線に近づく）

・手元のモデルがデータに対して過学習しているか、未学習なのかの判断
  1. 訓練誤差もテスト誤差もどちらも小さい場合は、うまく汎化されている可能性が高い
  2. 訓練誤差は小さいが、テスト誤差が大きい場合は、過学習を起こしている
  （2018年に出た深層学習の論文では、それでも学習を続けるとテスト誤差が小さくなっていくことが観測されてはいるが…）
  3. 訓練誤差もテスト誤差もどちらも小さくならない場合は、未学習に留まっている

●ホールドアウト法（テストデータと学習データの分割）
→有限のデータをテスト用と学習用に分割し、予測精度や誤り率を推定するために使用する
  基本的には一度分割したデータは固定する。
  ・学習用を増やせばテスト不足、テスト用を増やせば学習不足のジレンマ
  ・手元に大量のデータがないと、良い性能評価をえられない
  ・外れ値など少数データがテストデータのみに存在してしまう可能性がある（究極的にマズい）

●クロスバリデーション
→ホールドアウト法を、テストデータと学習データの分割パターンを変えて繰り返す

●グリッドサーチ
→すべてのチューニングパラメータの組み合わせで評価値を算出し、最もよい評価値のパラメータを採用する


【実装演習】
線形回帰、非線形回帰の実装を実際にやってみる

```
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.linear_model import LinearRegression
from sklearn.kernel_ridge import KernelRidge
```
```
# 非線形の分布になるようにデータを生成
n = 100; N = 2000

x = np.linspace(-3, 3, n)
X = np.linspace(-3, 3, N)

pix = np.pi * x
y = np.sin(pix) / pix + 0.05 * x + 0.1 * np.random.randn(n)

x = x.reshape(-1, 1)
X = X.reshape(-1, 1)
y = y.reshape(-1, 1)

plt.scatter(x,y)
```

![データ分布](https://github.com/kikukiku2004/rabbit/blob/main/report/Stage2_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/img/02_plot1.png)

```
# =====================
# 線形回帰でやってみる
# =====================
# モデル生成
model = LinearRegression()
model.get_params()
model.fit(x, y)

p = model.predict(X)

# プロット
plt.scatter(x, y)
plt.plot(X,p)
```

![線形回帰](https://github.com/kikukiku2004/rabbit/blob/main/report/Stage2_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/img/02_plot2.png)

```
# スコア
print(model.score(x, y))
```
```
# =====================
# 非線形回帰でやってみる
# =====================
# モデル生成
model2 = KernelRidge(alpha=0.001, kernel='rbf')
model2.fit(x, y)

p2 = model2.predict(X)

# プロット
plt.scatter(x, y)
plt.plot(X,p2)
```

![非線形回帰](https://github.com/kikukiku2004/rabbit/blob/main/report/Stage2_%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/img/02_plot3.png)

```
# スコア
print(model2.score(x, y))
```

【要点まとめ】

サポートベクターマシン
  ・分類問題、回帰問題のどちらにも適用が可能
  ・平面を線形な決定境界で区切ることで二値分類を行う。この時、学習データの最も決定境界に近いものと決定境界との距離をマージンと呼び、そのマージンを最大化する決定境界を探す
  ・ソフトマージン、ハードマージンがある。
    ソフトマージン：マージンの内側に一部のデータが入り込むことは許容する
    ハードマージン：マージンの内側にデータが入り込むことを許容しない


【実装演習】
乳がんの診断で、ロジスティック回帰とサポートベクターマシンの結果を比較する
```
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

data = pd.read_csv('./data/cancer.csv')
```
```
# radius_meanからnoname手前のfractal_dimension_worstまで３０次元を使用する
X = data.loc[:, "radius_mean":"fractal_dimension_worst"]
y = data['diagnosis'] = data['diagnosis'].map({'B' : 0, 'M' : 1}).astype(int)
```
```
# テストデータと訓練データに分割
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 5)
```
```
# ロジスティック回帰で学習
model = LogisticRegression(max_iter=2000)
model.fit(X_train, y_train)
```
```
# スコアを算出
logistic_score = model.score(X_test, y_test)
```
```
# サポートベクターマシンで学習
model2 = SVC()
model2.fit(X_train, y_train)
```
```
# スコアを算出
svc_score = model2.score(X_test, y_test)
```
```
logistic_score
```
```
0.9790209790209791
```
```
svc_score
```
```
0.951048951048951
```

【考察】
次元数は30と多いが、サポートベクターマシンでもロジスティック回帰と大きく違わない（2%）結果となった。
よりハードマージンに近づけた際に更に改善できるのかは検証していきたい。
【要点まとめ】

サポートベクターマシン
  ・分類問題、回帰問題のどちらにも適用が可能
  ・平面を線形な決定境界で区切ることで二値分類を行う。この時、学習データの最も決定境界に近いものと決定境界との距離をマージンと呼び、そのマージンを最大化する決定境界を探す
  ・ソフトマージン、ハードマージンがある。
    ソフトマージン：マージンの内側に一部のデータが入り込むことは許容する
    ハードマージン：マージンの内側にデータが入り込むことを許容しない


【実装演習】
乳がんの診断での次元数削減を検証する
```
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA

data = pd.read_csv('./data/cancer.csv')
```
```
# radius_meanからnoname手前のfractal_dimension_worstまで３０次元を使用する
X = data.loc[:, "radius_mean":"fractal_dimension_worst"]
y = data['diagnosis'] = data['diagnosis'].map({'B' : 0, 'M' : 1}).astype(int)
```
```
# テストデータと訓練データに分割
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 5)
```
```
# 学習
model = LogisticRegression(max_iter=2000)
model.fit(X_train, y_train)
```
```
# 30次元でのスコアを算出
dimention_30 = model.score(X_test, y_test)
```
```
# 次元数を2まで削減してみる
pca = PCA(n_components=2)
X_train2 = pca.fit_transform(X_train)
X_test2 = pca.fit_transform(X_test)
```
```
# 学習
model.fit(X_train2, y_train)
```
```
# 2次元でのスコアを算出
dimention_2 = model.score(X_test2, y_test)
```
```
dimention_30
```
```
0.9790209790209791
```
```
dimention_2
```
```
0.8811188811188811
```

【考察】
30次元では97%のスコア、2次元まで削減すると88%のスコアとなった。
9%の違いが大きく響くかは要件次第かもしれないが、大幅に次元を削減しても問題ないケースがあることがわかった。